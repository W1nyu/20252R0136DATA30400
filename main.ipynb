{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "540d381e-ca89-468e-9bd1-aa2c71c561dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T11:15:10.961992Z",
     "iopub.status.busy": "2025-12-19T11:15:10.961846Z",
     "iopub.status.idle": "2025-12-19T11:15:16.024413Z",
     "shell.execute_reply": "2025-12-19T11:15:16.023741Z",
     "shell.execute_reply.started": "2025-12-19T11:15:10.961976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.12/site-packages (5.2.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "# 1. 기본 설정\n",
    "!pip install sentence-transformers\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import TqdmWarning\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=TqdmWarning)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BASE_PATH = \"/home/sagemaker-user/project_release/Amazon_products\"\n",
    "TRAIN_CORPUS_PATH = os.path.join(BASE_PATH, \"train/train_corpus.txt\")\n",
    "TEST_CORPUS_PATH = os.path.join(BASE_PATH, \"test/test_corpus.txt\")\n",
    "CLASSES_PATH = os.path.join(BASE_PATH, \"classes.txt\")\n",
    "HIERARCHY_PATH = os.path.join(BASE_PATH, \"class_hierarchy.txt\")\n",
    "KEYWORDS_PATH = os.path.join(BASE_PATH, \"class_related_keywords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52adbf65-686d-4a22-9d03-88914a1dc1cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T11:15:16.025320Z",
     "iopub.status.busy": "2025-12-19T11:15:16.024980Z",
     "iopub.status.idle": "2025-12-19T11:15:16.070421Z",
     "shell.execute_reply": "2025-12-19T11:15:16.069822Z",
     "shell.execute_reply.started": "2025-12-19T11:15:16.025302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 29487\n",
      "Test Size: 19658\n",
      "Total Classes: 531\n",
      "Hierarchy Edges: 568\n"
     ]
    }
   ],
   "source": [
    "# 2. 데이터 로드\n",
    "def load_txt(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "    return lines\n",
    "\n",
    "def load_hierarchy(path):\n",
    "    parents = {}\n",
    "    children = {}\n",
    "    edges = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts: continue\n",
    "\n",
    "            if len(parts) >= 2:\n",
    "                p, c = parts[0], parts[1]\n",
    "                edges.append((p, c))\n",
    "    return edges\n",
    "\n",
    "train_texts = load_txt(TRAIN_CORPUS_PATH)\n",
    "test_texts = load_txt(TEST_CORPUS_PATH)\n",
    "class_names = load_txt(CLASSES_PATH)\n",
    "class_keywords_raw = load_txt(KEYWORDS_PATH)\n",
    "\n",
    "# 클래스 매핑\n",
    "class_to_id = {name: idx for idx, name in enumerate(class_names)}\n",
    "id_to_class = {idx: name for idx, name in enumerate(class_names)}\n",
    "\n",
    "# 키워드 매핑\n",
    "class_keywords = {}\n",
    "if len(class_keywords_raw) == len(class_names):\n",
    "    for idx, keywords in enumerate(class_keywords_raw):\n",
    "        class_keywords[idx] = keywords \n",
    "else:\n",
    "    for idx in range(len(class_names)):\n",
    "        class_keywords[idx] = class_names[idx]\n",
    "\n",
    "# 계층 구조 그래프 생성\n",
    "hierarchy_edges = load_hierarchy(HIERARCHY_PATH)\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(range(len(class_names)))\n",
    "\n",
    "for p_str, c_str in hierarchy_edges:\n",
    "    # 파일이 ID\n",
    "    if p_str.isdigit() and c_str.isdigit():\n",
    "        p_id, c_id = int(p_str), int(c_str)\n",
    "        G.add_edge(p_id, c_id)\n",
    "    # 파일이 텍스트 이름\n",
    "    elif p_str in class_to_id and c_str in class_to_id:\n",
    "        G.add_edge(class_to_id[p_str], class_to_id[c_str])\n",
    "\n",
    "print(f\"Train Size: {len(train_texts)}\")\n",
    "print(f\"Test Size: {len(test_texts)}\")\n",
    "print(f\"Total Classes: {len(class_names)}\")\n",
    "print(f\"Hierarchy Edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e86f6a-59ee-4ec3-9530-49d1e9813461",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T11:15:16.071219Z",
     "iopub.status.busy": "2025-12-19T11:15:16.071027Z",
     "iopub.status.idle": "2025-12-19T11:15:36.577777Z",
     "shell.execute_reply": "2025-12-19T11:15:36.577038Z",
     "shell.execute_reply.started": "2025-12-19T11:15:16.071205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e2f6cda4b546a2b5efe905105a774d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Train Texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f218c72b70c045d79a727b8a26bcf02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/461 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Silver Labels\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ea5940d46f443caefda34b825c7ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. SBERT 임베딩을 활용한 Silver Label 생성\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# 클래스 임베딩 생성\n",
    "enriched_class_texts = [f\"{name}: {class_keywords.get(idx, '')}\" for idx, name in enumerate(class_names)]\n",
    "class_embeddings = sbert_model.encode(enriched_class_texts, convert_to_tensor=True)\n",
    "\n",
    "# 학습 데이터 텍스트 임베딩 생성\n",
    "print(\"Encoding Train Texts\")\n",
    "train_embeddings = sbert_model.encode(train_texts, convert_to_tensor=True, show_progress_bar=True, batch_size=64)\n",
    "\n",
    "# 유사도 계산 및 Core Class 선정\n",
    "print(\"Generating Silver Labels\")\n",
    "cos_scores = util.cos_sim(train_embeddings, class_embeddings)\n",
    "\n",
    "silver_labels = []\n",
    "THRESHOLD = 0.3\n",
    "\n",
    "for i in tqdm(range(len(train_texts))):\n",
    "    scores = cos_scores[i]\n",
    "    \n",
    "    # Top-1 Class 추출\n",
    "    top_score, top_class_id = torch.max(scores, dim=0)\n",
    "    top_class_id = top_class_id.item()\n",
    "    top_score = top_score.item()\n",
    "    \n",
    "    current_labels = set()\n",
    "    \n",
    "    # 유사도 기반 필터링\n",
    "    current_labels.add(top_class_id)\n",
    "    ancestors = nx.ancestors(G, top_class_id)\n",
    "    current_labels.update(ancestors)\n",
    "\n",
    "    silver_labels.append(list(current_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b91c325a-589e-464d-b31f-a46647b37bbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T11:15:36.578766Z",
     "iopub.status.busy": "2025-12-19T11:15:36.578272Z",
     "iopub.status.idle": "2025-12-19T11:15:37.143125Z",
     "shell.execute_reply": "2025-12-19T11:15:37.142350Z",
     "shell.execute_reply.started": "2025-12-19T11:15:36.578734Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. Dataset 및 DataLoader 생성\n",
    "class AmazonReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.num_classes = len(class_names)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label_ids = self.labels[idx]\n",
    "\n",
    "        # Multi-hot encoding\n",
    "        target = torch.zeros(self.num_classes, dtype=torch.float)\n",
    "        for lid in label_ids:\n",
    "            target[int(lid)] = 1.0\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': target\n",
    "        }\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_dataset = AmazonReviewDataset(train_texts, silver_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc21cc2-cbfe-4dab-85bc-0608e58ec7ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T11:15:37.143761Z",
     "iopub.status.busy": "2025-12-19T11:15:37.143605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad0eddd7e564638b6d56bcd33aac2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss:  0.06468427177203452\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812efb897005495f9432c5d0b7d0dbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# 5. 모델 학습\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n",
    "model = BERTClass(len(class_names))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 학습 반복\n",
    "EPOCHS = 6 \n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for data in loop:\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    print(f\"Epoch: {epoch+1}, Average Loss:  {total_loss/len(train_loader)}\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e240b4-8dd6-44d2-aa1d-4a903cd981ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 추론 및 생성\n",
    "class AmazonTestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "test_dataset = AmazonTestDataset(test_texts, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"Predicting Test Data\")\n",
    "model.eval()\n",
    "final_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader):\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        \n",
    "        outputs = model(ids, mask)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "        \n",
    "        for i in range(len(probs)):\n",
    "            pred_indices = np.where(probs[i] > 0.3)[0] # Threshold\n",
    "            \n",
    "            # 예측값이 하나도 없으면 가장 확률 높은 Top 1 선택\n",
    "            if len(pred_indices) == 0:\n",
    "                pred_indices = [np.argmax(probs[i])]\n",
    "            \n",
    "            pred_set = set(pred_indices)\n",
    "            \n",
    "            # 계층 구조 후처리\n",
    "            ancestors_to_add = set()\n",
    "            for pid in pred_set:\n",
    "                ancestors_to_add.update(nx.ancestors(G, pid))\n",
    "            \n",
    "            pred_set.update(ancestors_to_add)\n",
    "            \n",
    "            final_predictions.append(list(pred_set))\n",
    "\n",
    "print(\"Prediction Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7849905a-4aeb-49c3-9e6d-0aa3aa2c1075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. CSV 저장\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "submission_path = \"submission.csv\"\n",
    "\n",
    "with open(submission_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'label'])\n",
    "    \n",
    "    for idx, preds in enumerate(final_predictions):\n",
    "        label_str = \",\".join(map(str, sorted(preds)))\n",
    "        writer.writerow([idx, label_str])\n",
    "\n",
    "print(f\"Submission file saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
